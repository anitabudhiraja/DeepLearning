{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled15.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNT7EpL5xVT6raTBUgnJGbv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anitabudhiraja/DeepLearning/blob/main/ANN_vs_CNN_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMXjAC3CFhiC"
      },
      "source": [
        "One major advantage of using CNNs over NNs is that you do not need to flatten the input images to 1D as they are capable of working with image data in 2D. This helps in retaining the “spatial” properties of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InKX0f2xH3SO",
        "outputId": "f6ed3bec-27b0-4c8a-9c83-e1725d8b78b4"
      },
      "source": [
        "#using ann\n",
        "# keras imports for the dataset and building our neural network\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "# Flattening the images from the 28x28 pixels to 1D 787 pixels\n",
        "X_train = X_train.reshape(60000, 784)\n",
        "X_test = X_test.reshape(10000, 784)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# normalizing the data to help with the training\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# one-hot encoding using keras' numpy-related utilities\n",
        "n_classes = 10\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
        "\n",
        "# building a linear stack of layers with the sequential model\n",
        "model = Sequential()\n",
        "# hidden layer\n",
        "model.add(Dense(100, input_shape=(784,), activation='relu'))\n",
        "# output layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# looking at the model summary\n",
        "model.summary()\n",
        "# compiling the sequential model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "# training the model for 10 epochs\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape before one-hot encoding:  (60000,)\n",
            "Shape after one-hot encoding:  (60000, 10)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_2 (Dense)              (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 79,510\n",
            "Trainable params: 79,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - 3s 5ms/step - loss: 2.1194 - accuracy: 0.4433 - val_loss: 1.2478 - val_accuracy: 0.7476\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 1.0860 - accuracy: 0.7702 - val_loss: 0.7266 - val_accuracy: 0.8374\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.6834 - accuracy: 0.8393 - val_loss: 0.5407 - val_accuracy: 0.8661\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.5231 - accuracy: 0.8690 - val_loss: 0.4537 - val_accuracy: 0.8823\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4481 - accuracy: 0.8822 - val_loss: 0.4005 - val_accuracy: 0.8948\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.4021 - accuracy: 0.8909 - val_loss: 0.3677 - val_accuracy: 0.9019\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3723 - accuracy: 0.8973 - val_loss: 0.3448 - val_accuracy: 0.9056\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3569 - accuracy: 0.9004 - val_loss: 0.3294 - val_accuracy: 0.9084\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3405 - accuracy: 0.9041 - val_loss: 0.3173 - val_accuracy: 0.9106\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 2s 4ms/step - loss: 0.3223 - accuracy: 0.9087 - val_loss: 0.3078 - val_accuracy: 0.9131\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2d402cfe90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI1tfsJ_ICUX"
      },
      "source": [
        "#Using CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwC0fWbeIhaw",
        "outputId": "4393d6f2-4bea-45d8-8172-a16225d345d4"
      },
      "source": [
        "# keras imports for the dataset and building our neural network\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# to calculate accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# loading the dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# building the input vector from the 28x28 pixels\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "# normalizing the data to help with the training\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# one-hot encoding using keras' numpy-related utilities\n",
        "n_classes = 10\n",
        "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
        "Y_train = np_utils.to_categorical(y_train, n_classes)\n",
        "Y_test = np_utils.to_categorical(y_test, n_classes)\n",
        "print(\"Shape after one-hot encoding: \", Y_train.shape)\n",
        "\n",
        "# building a linear stack of layers with the sequential model\n",
        "model = Sequential()\n",
        "# convolutional layer\n",
        "model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
        "model.add(MaxPool2D(pool_size=(1,1)))\n",
        "# flatten output of conv\n",
        "model.add(Flatten())\n",
        "# hidden layer\n",
        "model.add(Dense(100, activation='relu'))\n",
        "# output layer\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# compiling the sequential model\n",
        "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "\n",
        "# training the model for 10 epochs\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=10, validation_data=(X_test, Y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape before one-hot encoding:  (60000,)\n",
            "Shape after one-hot encoding:  (60000, 10)\n",
            "Epoch 1/10\n",
            "469/469 [==============================] - 44s 91ms/step - loss: 0.3790 - accuracy: 0.8910 - val_loss: 0.0700 - val_accuracy: 0.9783\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.0588 - accuracy: 0.9831 - val_loss: 0.0535 - val_accuracy: 0.9812\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 38s 82ms/step - loss: 0.0317 - accuracy: 0.9908 - val_loss: 0.0578 - val_accuracy: 0.9822\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 38s 82ms/step - loss: 0.0194 - accuracy: 0.9943 - val_loss: 0.0567 - val_accuracy: 0.9814\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 38s 82ms/step - loss: 0.0138 - accuracy: 0.9961 - val_loss: 0.0469 - val_accuracy: 0.9843\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 0.0543 - val_accuracy: 0.9836\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.0527 - val_accuracy: 0.9846\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.0035 - accuracy: 0.9992 - val_loss: 0.0755 - val_accuracy: 0.9815\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.0050 - accuracy: 0.9986 - val_loss: 0.0698 - val_accuracy: 0.9794\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 39s 83ms/step - loss: 0.0052 - accuracy: 0.9984 - val_loss: 0.0613 - val_accuracy: 0.9835\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2d3ef838d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}